---
created: 2025-09-04T10:44:25.2525+00:00
modified: 2025-09-04T10:50:52.5252+00:00
---
Самый используемый метод обучения нейронных сетей. С различными модификациями широко используется для глубоких нейронных сетей, персектронных. 
> [!warning] Важно 
> Известен также как **метод обратного распространения ошибки**. 

Градиентный спуск - метод численной оптимизации, который может быть использован во многих алгоритмах. 

Суть: есть некий процесс, который описывается функцией. Нужно подобрать такой набор аргументов функции, чтобы максимально точно восстановить исходную функцию. Используется во многих алгоритмах, где нужно найти экстремум функций. В частности:
* Нейронные сети
* Метод k-средних
* Регрессии
* И т.д.

Метод нахождения функции минимальных потерь.
> [!warning] Важно
>  Существует много методов этой функции

Минимизация любой функции сводится к поиску самой глубокой впадины этой функции. Используется чтобы контролировать ошибки в прогнозах моделей машинного обучения. Поиск минимума - получение минимально возможной ошибки или повышение точности модели. По сути, повышаем точность перебирая набор учебных данных. 
> [!warning] Важно
> То есть, *градиентный спуск* служит для минимизации функции потерь. В целом любой метод оптимизации может ориентироваться как на максимизацию процесса, так и для снижения функции потерь. В любом случае мы ищем *экстремум*


>[!info] Пример
>Предположим, что человек находится в горах. На основе данных, например крутизны, с использованием метода градиентного спуска человек может как найти вершину горы, так и найти впадину в которой он сыграет в неудавшегося кейвдайвера. Путь, пройденный вниз с горы - набор параметров. 

Основная проблема состоит в том, что для различных функций в различных условиях быстродействие ряда оптимизационных алгоритмов бывает недостаточным. В связи с этим функцию стоимости минимизирует с помощью численных методов. Обозначим функцию потерь $S(\theta)$ как сумма разностей $y_i$ и $\overline{y_i}$.

$S(\theta) = \sum^n_{i=0}(y_i - \overline{y_i})^2$

одно из свойств градиента - он указывает направление, в котором функция $F$ растёт или убывает быстрее всего. 

Возьмём некоторую точку $a$, в окрестности которой $F$ должна быть определена и дифференцируема. Тогда вектор антиградиента будет указывать на направление, в котором $F$ убывает быстрее всего. Отсюда делается вывод, что в некоторой точке $b = a - \alpha\triangleF(a)$ при некотором малом $\alpha$ значение функции будет $\leq$ значению точки $a$. Сделав шаг, следующая позиция будет меньше чем предыдущая. На каждом шаге позиция будет как минимум не увеличиваться.  Исходя из этих определений можно получить формулу нахождения неизвестных параметров:

$\theta_j = \theta_j - \alpha\frac{\delta S(\theta)}{\delta \theta_j}$
$\alpha$ в методах машинного обучения - скорость машинного обучения.

> [!info] Пример
> Необходимо минимизировать функцию $y = (\theta - 5) ^ 2$
> Минимизация - это нахождение $\theta$ при котором $y(\theta)$ будет минимально. 
> 
> 1) Находим производную по $\theta$: 
>    $\frac{dy}{d\theta} = 2(\theta - 5)$
>  2) Пусть изначально $\theta = 0$
>  3) Установим размер шага $\alpha = 0.1, 0.9, 1.2$
>  4) 25 раз подряд выполним раннее указанную формулу: 
>     $\theta^n=\theta^{n-1}-\alpha\frac{dy}{d\theta}$
>  5) Получаем $\theta$
>     
>    При $1.2$ в примере метод расходится, переходя выше и устремляясь в бесконечность.

$\alpha$ в простой реализации подбирается вручную и зависит от исходных данных.

>[!info] Пример
> На ненормализованных значениях с большим градиентом $\alpha = 0.001$ может приводить к расхождению. 

>[!info] Пример с плохой функцией
>$y = (x - 5) ^ 2 + 50sin(x) + 50$. При $\alpha = 0.1$ метод расходится и долго блуждает по холмам. Во втором случае ($\alpha = 0.01$) был найден глобальный минимум и варьируя значение скорости не выйдет заставить найти минимум локальный.

 Этот факт - один из недостатков метода. Он может гарантированно найти глобальный минимум только если функция выпуклая и гладкая. И если повезёт с начальными значениями.

> [!info] Пример с трёхмерной функцией
> Возьмём параболоид вращения $z = 4x^2 +16y^2$. Обратим внимание на то что все линии градиента направлены перпендикулярно изолиниями. Из этого следует что двигаясь в сторону антиградиента не выйдет за один шаг сразу же перепрыгнуть на минимум. Поскольку градиент указывает совсем не туда. Найдём формулу для вычисления неизвестных параметров $\theta_j$ линейной регрессии с методом наименьших квадратов
> ![[Основы ИИ/Заметки/Изображения/Pasted image 20250904131915.png]]
> 

Если бы количество элементов в текстовой выборке было равно единице, то можно было бы использовать формулу в изначальном виде. Однако же для случая когда в наличии n элементов алгоритм выглядит следующим образом:

Повторять v раз вычисления по формуле 
   ![[Основы ИИ/Заметки/Изображения/Pasted image 20250904133152.png]]
для каждого $j$ одновременно. Где n - количество элементов в обучающей выборке, а v - количество итераций.  

В свою очередь требование одновременности означает что производная должна быть вычислена со старыми значениями $\theta$. Иначе говоря, не стоит вычислять отдельно 1 параметр, затем - второй и т.д. Поскольку после изменения первого параметра отдельно производная также изменит своё значение. 

Если же вычислять значения параметров по одиночке, то это будет уже *не [[Основы ИИ/Заметки/Методы градиентного спуска/Градиентный спуск|градиентный спуск]]*. Представим, что имеется трёхмерная фигура, и если вычислять параметры один за одним, это можно представить как процесс движения только по одной координате за раз. То есть, один шаг по координате x, потом - по y и по z (ступеньками) вместо движения по вектору антиградиента. Приведённый выше вид называется пакетным градиентным спуском. Здесь количество повторений можно заменить на "повторять пока не сойдётся". 

Отсюда следует, что параметры будут корректироваться до тех пор, пока предыдущее и текущее значение стоимости не сравняются. Это значит что локальный/глобальный минимум найден и дальше алгоритм не пойдёт. Однако в реальных задачах достичь равенства практически невозможно. Поэтому вводится предел сходимости $\epsilon$. Его устанавливают какой-нибудь маленькой величиной и критерием остановки алгоритма является то, что разность текущего и предыдущего значений по модулю $< \epsilon$. То есть очевидно, что чем выше точность (меньше $\epsilon$), тем больше итераций алгоритма. 